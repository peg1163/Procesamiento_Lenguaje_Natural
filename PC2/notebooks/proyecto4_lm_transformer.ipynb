{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faa2d6c",
   "metadata": {},
   "source": [
    "# Practica calificada 2 \n",
    "## LM Neuronal con Transformer\n",
    "### * Beneficios del weight tying.\n",
    "    Usamos weight tying; reduce par치metros del softmax y mejora generalizaci칩n. En nuestro setup peque침o ayuda a no sobreajustar\n",
    "### * Transformers vs RNN en modelado de lenguaje.\n",
    "    las rnn procesan secuancialmente  y tienen poca paralelizacion aparte que pueden caer en la desaparicion de la gradiente \n",
    "    mientras que los trnasformers la autoatencion caota algunas dependencias cortas y tambien largas son mas rapidas de entrenar y escalan mejor \n",
    "    pero tienen una dificultad , generalmente son mas costosas computacionalmente , pero cuando hablamos de corpus peque침os un ngramaa le puede ganar a \n",
    "    un transformer como veremos en la ejecucion del codigo \n",
    "### * Exploding gradients y clipping.\n",
    "    en redes grandes el gradiente puede crecer mucho o desaparecer , esto hace que los pesos sean inestables  , por lo que el clipping limita la norma del gradiente\n",
    "    asi avita que el optimizador de \"pasos gigantes\" haciendo que se estabilice el entrenamiento  \n",
    "### * Impacto de la profundidad del modelo.\n",
    "    mas capas dentro de un modelo podria ayudar a mejorar la perplexidad ya que tienen mayor capacidad pero mas capas incrementan el costo tambien al tener mas de esto \n",
    "    se dificulta la optimizacion y el riesgo a que se sobreajuste "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
